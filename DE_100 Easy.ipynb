{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "nbkf4422ssulp7mp6uyp",
   "authorId": "7489729717373",
   "authorName": "YSAKURAGI",
   "authorEmail": "yuki.sakuragi@snowflake.com",
   "sessionId": "cd1e3dfd-a904-4a5f-95c9-43819cc4d979",
   "lastEditTime": 1755456624656
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8ad35e-7d1c-4aee-9f54-6949d9aa3afe",
   "metadata": {
    "name": "Intro1",
    "collapsed": false
   },
   "source": "## Avalanche (架空のウィンタースポーツ用品会社)\n\n![IMAGE](https://lh3.googleusercontent.com/pw/AP1GczPiLLf_4vKqdLeP8xr1GYa4eMa36fYztaHgEmiV94zrOvEsvIcPNWQnr85TIbzktK-fWbx32HgSryaWaaWJZjV35JU8E3krcwepmeQoW19s7UyloBZ4cOMTe-a0zCEz8hRMV1Kg4TM7cyEj13WdVAO2=w960-h540-s-no-gm?authuser=0)\n\n\nAvalancheの注文履歴・出荷データを, [Snowflake 上で動作する pandas](https://docs.snowflake.com/en/developer-guide/snowpark/python/pandas-on-snowflake) を使って分析します"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "imports"
   },
   "source": "# Snowpark Pandas API\nimport modin.pandas as spd\n# Import the Snowpark pandas plugin for modin\nimport snowflake.snowpark.modin.plugin\nimport streamlit as st\nfrom snowflake.cortex import sentiment, translate\n\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark.functions import ai_agg\nfrom snowflake.snowpark.context import get_active_session",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "snowpark_session"
   },
   "source": "# Snowflake のアクティブなセッション（現在接続中のセッション）を取得する\n# これによって、以降の処理で Snowflake に対して SQL 実行やデータ操作ができるようになる\nsession = get_active_session()\n\n# セッションに「クエリタグ (query tag)」を設定する\n# クエリタグとは、Snowflake 上で実行した SQL クエリに「ラベル」を付ける仕組み\n# これにより、モニタリングやトラブルシューティングで\n# 「どのアプリから来たクエリか」「どのハンズオン教材からの実行か」などを追跡できる\nsession.query_tag = {\n    \"origin\": \"sf_devrel\",          # クエリの発行元（ここでは Snowflake Developer Relations の意味）\n    \"name\": \"de_100_vhol\",          # このハンズオンや演習の名前\n    \"version\": {                    # バージョン情報\n        \"major\": 1,\n        \"minor\": 0\n    },\n    \"attributes\": {                 # 追加の属性情報（カスタムラベルのようなもの）\n        \"is_quickstart\": 1,         # Quickstart チュートリアルからの実行であることを示す\n        \"source\": \"notebook\",       # Jupyter Notebook や Snowflake Notebook からの実行であることを示す\n        \"vignette\": \"snowpark_pandas\"  # この教材のシナリオ名（Snowpark + pandas のハンズオンであること）\n    }\n}\n\n\n# ✨ ポイント\n# --------------\n# get_active_session() → すでに開いている Snowflake との接続を取ってくる関数。\n# query_tag → Snowflake に「このクエリは何のために動いたのか」を残せる便利なメタ情報。運用や監査で役立ちます。",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5109ca7b-e6e2-4fc2-bc33-801e1bebf29f",
   "metadata": {
    "name": "to_do_1",
    "collapsed": false
   },
   "source": "### TODO: ダウンロードした, 出荷データ(shipping-logs.csv)を Notebooks ワークスペースに読み込む\n- 画面左側の[➕]ボタンからファイルをアップロード"
  },
  {
   "cell_type": "code",
   "id": "db98d82e-5259-4e73-aabe-79bbb2b59c38",
   "metadata": {
    "language": "python",
    "name": "shipping_logs"
   },
   "outputs": [],
   "source": "# Snowpark pandas（spd）を使って CSV ファイルを読み込む\n# 'shipping-logs.csv' という名前のCSVファイルを対象にしている\n# CSVの中に 'shipping_date' という列があり、それを日付型（datetime型）として扱うよう指定している\nshipping_logs_mdf = spd.****(\n    'shipping-logs.csv',        # 読み込むCSVファイルの名前\n    parse_dates=['shipping_date']  # この列を「文字列」ではなく「日付」として読み込む\n)\n\n# 読み込んだデータ（shipping_logs_mdf）を表示する\n# shipping_logs_mdf は pandas.DataFrame と同じように扱えるオブジェクト\nshipping_logs_mdf\n\n\n# ✨ ポイント\n# --------------\n# ****\n#\n# parse_dates=['shipping_date'] → もし指定しないと \"2025-08-17\" のような日付も文字列（ただのテキスト）として扱われる。\n# → ここで指定することで「日付」として認識され、後で「日付ごとの集計」や「期間でフィルタ」などが簡単にできる。\n#\n# shipping_logs_mdf → 読み込んだデータを DataFrame 形式で保持する変数。\n# shipping_logs_mdf → 変数名の末尾 mdf は「modin dataframe（＝Snowpark pandasのDataFrame）」の略",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8be0fbae-e6ef-462e-8ddb-e575b59e3b74",
   "metadata": {
    "name": "to_do_2",
    "collapsed": false
   },
   "source": "### TODO: ダウンロードした, 注文履歴データ(order-history.csv)を Notebooks ワークスペースに読み込む\n- 画面左側の[➕]ボタンからファイルをアップロード"
  },
  {
   "cell_type": "code",
   "id": "17be293e-956d-4781-be26-bbbdd94afc97",
   "metadata": {
    "language": "python",
    "name": "order_history"
   },
   "outputs": [],
   "source": "# Snowpark pandas（spd）を使って CSV ファイルを読み込む\n# 'order-history.csv' という名前のCSVファイルを対象にしている\n# CSVの中に 'Date' という列があり、それを日付型（datetime型）として扱うように指定している\norder_history_mdf = spd.****(\n    'order-history.csv',   # 読み込むCSVファイルの名前\n    parse_dates=['Date']   # 'Date' 列を文字列ではなく「日付」として扱う\n)\n\n# 読み込んだデータ（order_history_mdf）を表示する\n# order_history_mdf は pandas.DataFrame と同じように扱えるオブジェクト\norder_history_mdf\n\n\n# ✨ ポイント\n# --------------\n# ****\n#\n# parse_dates=['Date'] → 「注文日（Date）」の列を日付型にしておくと、後で「月ごとの集計」「特定の期間の抽出」などが簡単にできる。",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f87b19c-07ca-4bec-8779-d6396126179b",
   "metadata": {
    "language": "python",
    "name": "rename_columns"
   },
   "outputs": [],
   "source": "# order_history_mdf の列名を分かりやすく変更する\n# ****(columns={...}) で「元の列名 : 新しい列名」を指定する\n\norder_history_mdf = order_history_mdf.****(columns = {\n    'Order ID': 'order_id',              # 注文ID → order_id\n    'Customer ID': 'customer_id',        # 顧客ID → customer_id\n    'Product ID': 'product_id',          # 商品ID → product_id\n    'Product Name': 'product_name',      # 商品名 → product_name\n    'Quantity Ordered': 'quantity_ordered',  # 注文数 → quantity_ordered\n    'Price': 'price',                    # 単価 → price\n    'Total Price': 'total_price',        # 合計金額 → total_price\n    'Date': 'date'                       # 日付 → date\n})\n\n# 列名が正しく変更されたかを確認する\norder_history_mdf.columns\n\n# ✨ ポイント\n#--------------\n# ****(columns={...}) → 辞書形式（キー: 値）で「古い名前 → 新しい名前」に変換する。\n#\n# スネークケース（例: order_id）は Python で一般的な書き方で、プログラムで扱いやすい。\n#\n# この処理をしておくと、後でコードを書くときに order_history_mdf[\"order_id\"] のように呼び出しやすくなる。",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "36e9f31f-f328-4b33-b567-fb879efa0f9e",
   "metadata": {
    "name": "remove_dollar",
    "collapsed": false
   },
   "source": "### 価格カラムから $ 記号を取り除いて整理する"
  },
  {
   "cell_type": "code",
   "id": "05b710b6-cee7-478e-9870-c2dd5c3119f8",
   "metadata": {
    "language": "python",
    "name": "clean_price_func"
   },
   "outputs": [],
   "source": "# 文字列で表現された価格（例: \"$19.99\"）を数値に変換する関数\ndef clean_price(price_str):\n    # 価格の文字列から \"$\" 記号を取り除き、前後の余分な空白も削除する\n    # 例: \" $19.99 \" → \"19.99\"\n    cleaned = price_str.****.strip()\n    \n    # 文字列になっている数値を float型（小数点を持つ数値）に変換する\n    # 例: \"19.99\" → 19.99\n    return float(cleaned)\n\n\n# ✨ ポイント\n# --------------\n# **** → $ を空文字に置き換えて削除する。\n#\n# .strip() → 文字列の前後にある余計なスペースや改行を削除する。\n#\n# float() → 文字列を「実数」に変換する。計算で使えるようになる。",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "db2da873-f6ed-4e62-9518-845dc99e5ac9",
   "metadata": {
    "language": "python",
    "name": "clean_up_price_values"
   },
   "outputs": [],
   "source": "# ---- 価格カラムを数値に変換する処理 ----\n\n# 'price' 列に対して clean_price 関数を適用する\n# これにより \"$19.99\" のような文字列が 19.99 という float型の数値になる\norder_history_mdf['price'] = order_history_mdf['price'].apply(clean_price)\n\n# 'total_price' 列に対しても同じく clean_price を適用する\n# これで合計金額も数値として扱えるようになる\norder_history_mdf['total_price'] = order_history_mdf['total_price'].apply(clean_price)\n\n\n# ---- 変換後のデータ型を確認する処理 ----\n\n# price 列のデータ型を表示（float になっていればOK）\nprint(\"\\nPrice column data type:\", order_history_mdf['price'].dtype)\n\n# total_price 列のデータ型を表示（こちらも float になっていればOK）\nprint(\"Total price column data type:\", order_history_mdf['total_price'].dtype)\n\n\n\n# ✨ ポイント\n# --------------\n# .apply(clean_price) → 各行の値に対して clean_price 関数を実行する。\n#\n# データ型の確認 (dtype)\n# 変換前 → object（文字列）\n# 変換後 → float64（小数点付き数値）\n#\n# こうしておくことで、後から「平均」「合計」「グラフ化」などの数値演算ができるようになる。\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "80ae513f-b06b-4a08-9bf5-e97db9a98eed",
   "metadata": {
    "language": "python",
    "name": "check_clean_up_price_values"
   },
   "outputs": [],
   "source": "# 実際に $ が消えて数値化されているかを表で確認\norder_history_mdf.head()\n\n\n# ✨ ポイント\n# --------------\n# .head() → データフレームの最初の5行を表示して内容を確認する\n# 読み込みや列名変更、数値変換が正しくできているかチェックするのに便利",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b7d9aaed-ab1c-482f-b87e-fa4c3955fdac",
   "metadata": {
    "name": "join_order_shipping",
    "collapsed": false
   },
   "source": "### 製品ごとの注文数を計算する：order_history と shipping_logs を結合する"
  },
  {
   "cell_type": "code",
   "id": "8ec8cd02-16c1-4999-8870-da25c1281f27",
   "metadata": {
    "language": "python",
    "name": "join_tables"
   },
   "outputs": [],
   "source": "# ---- 注文データと出荷データを結合 ----\n\n# order_history_mdf（注文データ）と shipping_logs_mdf（出荷データ）を\n# 'order_id' 列をキーにして結合（マージ）する\n\norder_shipping_mdf = spd.****(\n    order_history_mdf,      # 左側のデータフレーム（注文履歴）\n    shipping_logs_mdf,      # 右側のデータフレーム（出荷ログ）\n    on='order_id',          # 結合キーとなる列\n    how='****'             # 内部結合（両方に存在するデータのみ）\n)\n\n# 結合後のデータフレームの先頭5行を表示して確認\norder_shipping_mdf.head(5)\n\n\n# ✨ ポイント\n# --------------\n# spd.****() → pandas の **** と同じように使えるが、Snowpark pandas 上で動作する\n#\n# on='order_id' → 「注文ID」を使ってデータを紐付ける\n#\n# how='****'：\n#     両方のテーブルに存在する注文だけを残す\n#     片方だけにある注文は削除される\n#     結合すると、注文情報と出荷情報が 1行にまとめられる ので分析しやすくなる\n#         例) 「注文から出荷までの日数」や「商品ごとの売上と出荷状況」\n#\n# 外部結合（how='left' や how='right'）を使うと、片方にしかないデータも残せる\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1eb33443-f7c3-41a5-acfe-6cbe634ab990",
   "metadata": {
    "language": "python",
    "name": "product_order_counts"
   },
   "outputs": [],
   "source": "# ---- 商品ごとの注文件数を集計 ----\n\n# 'product_name' 列でグループ化して、注文件数を数える\n# ****() は各グループの行数（＝注文数）をカウントする\n# reset_index(name='order_count') で結果をデータフレーム形式に戻し、列名を 'order_count' に設定\nproduct_counts_mdf = order_shipping_mdf.****('product_name').****().reset_index(name='order_count')\n\n# ---- 注文件数の多い順に並べ替え ----\n\n# sort_values() で 'order_count' 列を降順（ascending=False）に並べる\nproduct_counts_mdf = product_counts_mdf.sort_values('order_count', ascending=False)\n\n# ---- 結果を表示 ----\nprint(\"\\nProduct Order Counts:\")\nst.dataframe(product_counts_mdf)\n\n\n\n# ✨ ポイント\n# --------------\n# ****('product_name') → 同じ商品ごとにまとめる \n#\n# .****() → 各商品の注文数をカウント\n#\n# .reset_index(name='order_count') → 集計結果をデータフレームとして整形し、列名を order_count に変更\n#\n# .sort_values(..., ascending=False) → 注文件数の多い順に並べ替える\n# --------------\n# これにより、どの商品が人気か（注文が多いか）が一目でわかる\n# もし上位5商品だけ見たい場合は product_counts_mdf.head(5) と書くと便利\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c92cd23f-deb0-41b3-91f5-7294a6ee6267",
   "metadata": {
    "name": "pivot",
    "collapsed": false
   },
   "source": "### 注文の配送ステータスごとにピボットする"
  },
  {
   "cell_type": "code",
   "id": "d803b11b-55e1-4b44-869f-e67d3c95639c",
   "metadata": {
    "language": "python",
    "name": "product_status_pivot"
   },
   "outputs": [],
   "source": "# ---- 商品ごとの注文ステータス別集計 ----\n\n# **** を使って集計\n# index='product_name' → 行に商品名を設定\n# columns='status' → 列に注文ステータス（例: shipped, pending, cancelled）を設定\n# values='order_id' → 注文IDを数える対象にする\n# aggfunc='count' → 各セルに注文数をカウント\n# fill_value=0 → データがない場合は 0 を埋める\nproduct_status_pivot_mdf = order_shipping_mdf.****(\n    index='product_name',\n    columns='status',\n    values='order_id',\n    aggfunc='count',\n    fill_value=0\n)\n\n# ---- 合計注文数の列を追加 ----\n\n# 行ごとの合計を計算して 'Total_Orders' 列として追加\nproduct_status_pivot_mdf['Total_Orders'] = product_status_pivot_mdf.sum(axis=1)\n\n# ---- 合計注文数の多い順に並べ替え ----\n\nproduct_status_pivot_mdf = product_status_pivot_mdf.sort_values('Total_Orders', ascending=False)\n\n# ---- 結果を表示 ----\nprint(\"\\nProduct Orders by Status:\")\nst.dataframe(product_status_pivot_mdf)\n\n# ✨ ポイント\n# --------------\n# **** → 行・列を指定して集計表（ピボットテーブル）を作る\n\n# aggfunc='count' → 注文数をカウント\n\n# sum(axis=1) → 行方向の合計を計算して「合計注文数」を追加\n\n# この表を作ると、\n# 商品ごとのステータス別の注文数が一目でわかる\n# 合計注文数で人気商品をすぐ把握できる",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "62e8654c-416d-4279-8c21-b53987491ca8",
   "metadata": {
    "name": "intro2",
    "collapsed": false
   },
   "source": "## Avalanche社は、各製品に対する顧客レビューについても理解したいと考えています。  \n\n![IMAGE](https://lh3.googleusercontent.com/pw/AP1GczMuP-pHWhjNDtQwRpMYm0FKey9xlDfRMvcSa6HhxnJrhG-oCs6ydlOhpCvR5VcNDjbFNRir_H4XsFaay-lehwzRV1pgKoB9DjJ31SduUCD2F1gwmZgG4SAM6vNseULS3tYZoW7taYzTW-gc5Lt-4gu3=w960-h540-s-no-gm?authuser=0)\n\n\n\nこの分析を [Snowpark DataFrame API](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes) を使って実行してみましょう。"
  },
  {
   "cell_type": "code",
   "id": "700a31fe-3a77-4966-bfef-c29cb3c1fe1a",
   "metadata": {
    "language": "sql",
    "name": "create_snowflake_objects"
   },
   "outputs": [],
   "source": "-- ---- データベースとスキーマの作成（Snowsight UI で実行する場合） ----\n-- CREATE OR REPLACE DATABASE avalanche_db;\n-- CREATE OR REPLACE SCHEMA avalanche_schema;\n\n-- 既存データベースを使用する\nUSE DATABASE avalanche_db;\n\n-- 既存スキーマを使用する\nUSE SCHEMA avalanche_schema;\n\n\n-- ---- ファイルを格納するステージ（Stage）の作成 ----\n-- Stage とは、Snowflake にデータを取り込む前に一時的にファイルを置いておく場所です\nCREATE OR REPLACE STAGE avalanche_stage\n  URL = 's3://sfquickstarts/misc/avalanche/csv/'  -- S3 バケットの場所を指定\n  DIRECTORY = (ENABLE = TRUE AUTO_REFRESH = TRUE); -- ディレクトリ構造を有効化、自動更新ON\n\n-- Stage 内のファイル一覧を確認\nls @avalanche_stage;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e0f6a184-8c2f-4d54-a606-fd3f2acc1d22",
   "metadata": {
    "name": "load_reviews",
    "collapsed": false
   },
   "source": "### 顧客レビューを Snowflake のテーブルに読み込む"
  },
  {
   "cell_type": "code",
   "id": "4903c319-f55a-4773-8e58-daaec82f72d3",
   "metadata": {
    "language": "sql",
    "name": "create_customer_reviews_table"
   },
   "outputs": [],
   "source": "-- ---- テーブルの作成 ----\n-- customer_reviews という名前のテーブルを作成\n-- 商品名、レビュー日、レビュー本文、感情スコアを保存する\nCREATE OR REPLACE TABLE customer_reviews (\n    product VARCHAR,          -- 商品名（文字列）\n    date DATE,                -- レビュー日（DATE型）\n    summary TEXT,             -- レビュー本文（TEXT型）\n    sentiment_score FLOAT     -- 感情スコア（数値、小数点）\n);\n\n\n-- ---- CSV ファイルからデータをテーブルにロード ----\nCOPY INTO customer_reviews\nFROM @avalanche_stage/customer_reviews.csv   -- 先ほど作成した Stage 内の CSV を指定\nFILE_FORMAT = (\n    TYPE = CSV,                               -- CSV形式のファイル\n    FIELD_DELIMITER = ',',                     -- カラム区切り文字はカンマ\n    SKIP_HEADER = 1,                           -- 1行目はヘッダーなのでスキップ\n    FIELD_OPTIONALLY_ENCLOSED_BY = '\"',       -- 値が \" \" で囲まれている場合に対応\n    TRIM_SPACE = TRUE,                         -- 前後の空白を削除\n    NULL_IF = ('NULL', 'null'),               -- \"NULL\" または \"null\" は NULL として扱う\n    EMPTY_FIELD_AS_NULL = TRUE                -- 空欄も NULL として扱う\n);\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d558d8e0-c499-4ae5-9ca0-85497fedc71a",
   "metadata": {
    "language": "python",
    "name": "load_customer_reviews"
   },
   "outputs": [],
   "source": "# ---- Snowflake テーブルを Snowpark DataFrame として読み込む ----\n\n# 'customer_reviews' テーブルを Snowpark DataFrame として取得\ncustomer_reviews_sdf = session.table('customer_reviews')\n\n# 取得した Snowpark DataFrame の内容を確認\ncustomer_reviews_sdf\n\n\n# ✨ ポイント\n# --------------\n# session.table('table_name')\n#   Snowflake 上の既存テーブルを Snowpark DataFrame として扱う\n#   pandas の DataFrame に似ているが、実際のデータは Snowflake にあり、クエリ実行時に必要な部分だけ取得する\n#\n# customer_reviews_sdf\n#   この変数に Snowpark DataFrame が格納される\n#   データの操作（フィルタリング、集計、結合など）を Snowflake 側で効率的に実行可能\n\n# 💡 補足\n# --------------\n# Snowpark DataFrame は Lazy Evaluation（遅延評価） です\n#   customer_reviews_sdf を定義しただけではまだデータは取得されない\n#   データを確認したい場合は .show() や .to_pandas() などで明示的に取得する必要があります",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d3eddc5a-cd65-4ceb-a20e-057eade19152",
   "metadata": {
    "language": "python",
    "name": "drop_sentiment"
   },
   "outputs": [],
   "source": "# ---- SENTIMENT_SCORE 列を削除 ----\n\n# ****() を使って列を削除\ncustomer_reviews_sdf = customer_reviews_sdf.****(F.col(\"SENTIMENT_SCORE\"))\n\n# 結果を表示（列が削除されていることを確認）\ncustomer_reviews_sdf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "26cba86a-f7fd-4804-a5c7-68bb223d8f21",
   "metadata": {
    "language": "python",
    "name": "cortex_sentiment"
   },
   "outputs": [],
   "source": "# ---- SUMMARY 列を使って感情スコアを計算し、新しい列に追加 ----\n\ncustomer_reviews_sdf = customer_reviews_sdf.****(\n    \"SENTIMENT_SCORE\",                          # 新しく追加する列名\n    ****(F.col(\"SUMMARY\"))  # Cortex の **** 関数で感情分析\n)\n\n# 結果を表示（sentiment_score 列が追加されていることを確認）\ncustomer_reviews_sdf\n\n\n# ✨ ポイント\n# --------------\n# ****(\"新しい列名\", 計算式)\n#   Snowpark DataFrame に新しい列を追加\n#   元の列を更新したい場合も同じ方法で上書き可能\n#\n# snowflake.cortex.****(F.col(\"SUMMARY\"))\n#   Snowflake Cortex AI 機能を使ってテキストの感情を数値化\n#   SUMMARY の文章を入力として、ポジティブ・ネガティブの度合いを数値化（感情スコア）\n# --------------\n# 💡 補足\n# これにより、レビュー本文を数値化して分析可能になる\n# 後で商品別の平均感情スコアを計算したり、可視化に使える\n# Cortex AI を使うため、Snowflake の Enterprise Edition や対応環境が必要",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d04445ac-2345-4bf9-a939-f81439e0b5e6",
   "metadata": {
    "name": "calc_avg_sentiment",
    "collapsed": false
   },
   "source": "### 注文された各商品ごとに感情スコアの平均値を計算する"
  },
  {
   "cell_type": "code",
   "id": "54df86c8-57ef-43b9-9f20-8b4700b0367a",
   "metadata": {
    "language": "python",
    "name": "calc_product_sentiment"
   },
   "outputs": [],
   "source": "# ---- 商品ごとの平均感情スコアを計算 ----\n\nproduct_sentiment_sdf = customer_reviews_sdf.group_by('PRODUCT') \\\n    .****(\n        # SENTIMENT_SCORE の平均を計算し、小数点2桁に丸める\n        F.round(F.avg('SENTIMENT_SCORE'), 2).alias('AVG_SENTIMENT_SCORE')\n    ).sort(F.col('AVG_SENTIMENT_SCORE').desc()) # 平均感情スコアの降順で並べ替え\n\n# ---- 結果を表示 ----\nprint(\"\\nAverage Sentiment Scores by Product:\")\nproduct_sentiment_sdf.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "731ae761-b354-4cc7-b986-9790428d6ec9",
   "metadata": {
    "name": "ai_agg_reviews",
    "collapsed": false
   },
   "source": "### 注文された各商品ごとにレビュー文を1センテンスに要約する\n"
  },
  {
   "cell_type": "code",
   "id": "4b09a69e-dec8-491e-a781-bed814e7ff76",
   "metadata": {
    "language": "python",
    "name": "calc_product_agg"
   },
   "outputs": [],
   "source": "# ---- 商品ごとの集計とレビュー要約 ----\n\nproduct_sentiment_sdf = customer_reviews_sdf.group_by('PRODUCT') \\\n    .****(\n        # SENTIMENT_SCORE 列の平均を計算し、小数点2桁に丸める\n        F.round(F.avg('SENTIMENT_SCORE'), 2).alias('AVG_SENTIMENT_SCORE'),\n        \n        # SUMMARY 列を AI で要約（1文にまとめる）\n        ****(F.col(\"SUMMARY\"), \"Summarize the customer reviews in one sentence.\").alias('SUMMARY')\n    ).sort(F.col('AVG_SENTIMENT_SCORE').desc()) # 平均感情スコアの降順で並べ替え（高評価の商品から表示）\n\n\n# ---- 要約結果を日本語に翻訳 ----\nproduct_sentiment_sdf = product_sentiment_sdf.with_column(\n    \"JP_SUMMARY\", \n    translate(F.col(\"SUMMARY\"), '', 'JA')  # 翻訳対象: SUMMARY 列, 翻訳先: 日本語（JA）\n)\n\n\n# ---- 結果を Streamlit で表示 ----\nprint(\"\\nAverage Sentiment Scores by Product:\")\nst.dataframe(product_sentiment_sdf)  # Streamlit のテーブル表示\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a36c9e6-66e3-4e91-b57b-5f5123680ee3",
   "metadata": {
    "language": "python",
    "name": "save_product_sentiment"
   },
   "outputs": [],
   "source": "# ---- Snowpark DataFrame を Snowflake のテーブルとして保存 ----\n\nproduct_sentiment_sdf.write.****(\n    'PRODUCT_SENTIMENT_ANALYSIS',  # 保存するテーブル名\n    mode='overwrite'               # 既存の同名テーブルがあれば上書き\n)\n\n\n# ✨ ポイント\n# --------------\n# テーブル名\n#   Snowflake 内で一意の名前を付ける\n#   大文字小文字は Snowflake 側で自動的に大文字扱いになる\n#\n# テーブルとして保存するメリット\n#   保存することで、後で SQL クエリや BI ツールから直接参照可能\n#   DataFrame を再計算する必要がなくなる",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "75ee1f78-c831-4cc7-8203-8898e61512e1",
   "metadata": {
    "name": "task",
    "collapsed": false
   },
   "source": "## Snowflake タスクを使って、分析を定期的に自動実行する\n"
  },
  {
   "cell_type": "markdown",
   "id": "58992757-6c5e-4de7-b8ee-041ff098ec7d",
   "metadata": {
    "name": "create_task"
   },
   "source": "-- ---- タスクの作成 ----\nCREATE OR REPLACE TASK avalanche_analysis_task\n    WAREHOUSE = COMPUTE_WH                                         -- タスク実行に使う仮想ウェアハウス\n    SCHEDULE = 'USING CRON 0 0 * * * America/New_York'            -- 毎日ニューヨーク時間で午前0時に実行\nAS\nBEGIN\n    -- ---- データベースとスキーマを作成（存在しない場合のみ） ----\n    CREATE DATABASE IF NOT EXISTS avalanche_test_db;\n    CREATE SCHEMA IF NOT EXISTS avalanche_test_db.avalanche_test_schema;\n    \n    -- ---- 作成したデータベースとスキーマを使用 ----\n    USE DATABASE avalanche_test_db;\n    USE SCHEMA avalanche_test_db.avalanche_test_schema;\n    \n    -- ---- ファイルを置くステージを作成 ----\n    CREATE OR REPLACE STAGE avalanche_stage\n        URL = 's3://sfquickstarts/misc/avalanche/csv/'        -- CSV ファイルの置き場所（S3）\n        DIRECTORY = (ENABLE = TRUE AUTO_REFRESH = TRUE);       -- ディレクトリ構造を有効化、自動更新ON\n    \n    -- ---- customer_reviews テーブルを作成 ----\n    CREATE OR REPLACE TABLE customer_reviews (\n        product VARCHAR,         -- 商品名\n        date DATE,               -- レビュー日\n        summary TEXT,            -- レビュー本文\n        sentiment_score FLOAT    -- 感情スコア\n    );\n    \n    -- ---- CSV ファイルからデータをロード ----\n    COPY INTO customer_reviews\n    FROM @avalanche_stage/customer_reviews.csv\n    FILE_FORMAT = (\n        TYPE = CSV,\n        FIELD_DELIMITER = ',',\n        SKIP_HEADER = 1,\n        FIELD_OPTIONALLY_ENCLOSED_BY = '\"',\n        TRIM_SPACE = TRUE,\n        NULL_IF = ('NULL', 'null'),\n        EMPTY_FIELD_AS_NULL = TRUE\n    );\n    \n    -- ---- 商品ごとの平均感情スコアを計算して保存 ----\n    CREATE OR REPLACE TABLE product_sentiment_analysis AS\n    SELECT \n        product,\n        ROUND(AVG(sentiment_score), 2) AS avg_sentiment_score\n    FROM customer_reviews\n    GROUP BY product\n    ORDER BY avg_sentiment_score DESC;\nEND;\n"
  },
  {
   "cell_type": "markdown",
   "id": "d53f6e6f-1eb2-449a-bf51-fd381aaab217",
   "metadata": {
    "name": "resume_task"
   },
   "source": "-- ---- 作成したタスクを有効化 ----\n\nALTER TASK avalanche_analysis_task RESUME;\n-- RESUME を実行するとタスクが「有効」になり、スケジュールに沿って自動実行される\n\n\n-- ✨ ポイント\n----------------\n-- ALTER TASK ... RESUME\n--  タスクを有効化するコマンド\n--  Snowflake でタスクは作成時点では 停止状態（SUSPENDED） になっているので、まず有効化する必要がある\n--\n-- 自動実行開始\n--  タスクが有効になると、SCHEDULE で指定したタイミング（今回なら毎日ニューヨーク時間の0時）で自動的に実行される\n--\n-- 手動実行との違い\n--  有効化だけでは即実行されない\n--  今すぐ実行したい場合は EXECUTE TASK avalanche_analysis_task; を別途実行する"
  },
  {
   "cell_type": "markdown",
   "id": "27c9b439-978c-48e4-b8a6-487a278dc72e",
   "metadata": {
    "name": "to_do_3",
    "collapsed": false
   },
   "source": "### TODO: タスクの実行履歴を確認し、スケジュールされたタスクをどのように監視できるか見てみましょう\n"
  }
 ]
}